{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0f4574",
   "metadata": {},
   "source": [
    "# CARLA Autonomous Vehicle with LiDAR-Camera Fusion\n",
    "\n",
    "**AI-controlled vehicle using LiDAR-to-camera projection for obstacle detection and navigation**\n",
    "\n",
    "## Pipeline\n",
    "1. Connect to CARLA in synchronous mode\n",
    "2. Clear environment (keep only roads)\n",
    "3. Spawn vehicle with cameras + LiDAR\n",
    "4. Setup sensor fusion with LiDAR projection\n",
    "5. Run AI-controlled navigation\n",
    "6. Save synchronized sensor data\n",
    "7. Cleanup and restore\n",
    "\n",
    "See `README.md` for detailed documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc85f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dependencies loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20594/1756308635.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  VIRIDIS = np.array(cm.get_cmap('viridis').colors)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import carla\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import queue\n",
    "from matplotlib import cm\n",
    "\n",
    "# Viridis colormap for LiDAR intensity visualization\n",
    "VIRIDIS = np.array(cm.get_cmap('viridis').colors)\n",
    "VID_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "\n",
    "print(\"‚úì Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a9a82",
   "metadata": {},
   "source": [
    "## 1. CARLA Connection & Synchronous Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to CARLA server\n",
    "client = carla.Client('localhost', 2000)\n",
    "client.set_timeout(20.0)\n",
    "print(\"‚úì Connected to CARLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get world and store original settings\n",
    "world = client.get_world()\n",
    "carla_map = world.get_map()\n",
    "original_settings = world.get_settings()\n",
    "\n",
    "# Enable synchronous mode for deterministic sensor capture\n",
    "settings = world.get_settings()\n",
    "settings.synchronous_mode = True\n",
    "settings.fixed_delta_seconds = 0.05  # 20 FPS\n",
    "world.apply_settings(settings)\n",
    "\n",
    "# Synchronize traffic manager\n",
    "traffic_manager = client.get_trafficmanager(8000)\n",
    "traffic_manager.set_synchronous_mode(True)\n",
    "\n",
    "print(f\"‚úì Map: {carla_map.name}\")\n",
    "print(f\"‚úì Synchronous mode: {1/settings.fixed_delta_seconds:.0f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e7080",
   "metadata": {},
   "source": [
    "## 2. Clear Environment (Keep Only Roads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all static map objects\n",
    "for layer in [carla.MapLayer.Buildings, carla.MapLayer.Decals, carla.MapLayer.Foliage,\n",
    "              carla.MapLayer.ParkedVehicles, carla.MapLayer.Particles, \n",
    "              carla.MapLayer.Props, carla.MapLayer.Walls]:\n",
    "    world.unload_map_layer(layer)\n",
    "world.tick()\n",
    "\n",
    "print(\"‚úì Environment cleared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a19349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload all static map objects\n",
    "for layer in [carla.MapLayer.Buildings, carla.MapLayer.Decals, carla.MapLayer.Foliage,\n",
    "              carla.MapLayer.ParkedVehicles, carla.MapLayer.Particles, \n",
    "              carla.MapLayer.Props, carla.MapLayer.Walls]:\n",
    "    world.load_map_layer(layer)\n",
    "world.tick()\n",
    "\n",
    "print(\"‚úì Environment reloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ac8bb",
   "metadata": {},
   "source": [
    "## 3. Spawn Vehicle & Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49062329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spawn vehicle at predetermined location for reproducibility\n",
    "blueprint_library = world.get_blueprint_library()\n",
    "vehicle_bp = blueprint_library.filter('vehicle.tesla.model3')[0]\n",
    "\n",
    "# Fixed spawn coordinates (x, y, z) and rotation (pitch, yaw, roll)\n",
    "# Using spawn point 0 from map\n",
    "SPAWN_X = -64.6\n",
    "SPAWN_Y = 24.5\n",
    "SPAWN_Z = 0.6\n",
    "SPAWN_YAW = 0.2  # Vehicle facing direction (degrees)\n",
    "\n",
    "spawn_point = carla.Transform(\n",
    "    carla.Location(x=SPAWN_X, y=SPAWN_Y, z=SPAWN_Z),\n",
    "    carla.Rotation(pitch=0, yaw=SPAWN_YAW, roll=0)\n",
    ")\n",
    "vehicle = world.spawn_actor(vehicle_bp, spawn_point)\n",
    "world.tick()\n",
    "\n",
    "print(f\"‚úì Vehicle spawned at fixed location ({SPAWN_X}, {SPAWN_Y}, {SPAWN_Z})\")\n",
    "print(f\"  Facing: {SPAWN_YAW}¬∞ yaw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c8c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure RGB cameras (1920x1080, 90¬∞ FOV)\n",
    "camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "camera_bp.set_attribute('image_size_x', '1920')\n",
    "camera_bp.set_attribute('image_size_y', '1080')\n",
    "camera_bp.set_attribute('fov', '90')\n",
    "\n",
    "# Camera positions: front, left, right\n",
    "camera_configs = {\n",
    "    'front': carla.Transform(carla.Location(x=2.5, z=1.0), carla.Rotation(pitch=0, yaw=0)),\n",
    "    'left': carla.Transform(carla.Location(x=0.0, y=-1.0, z=1.0), carla.Rotation(pitch=0, yaw=-90)),\n",
    "    'right': carla.Transform(carla.Location(x=0.0, y=1.0, z=1.0), carla.Rotation(pitch=0, yaw=90))\n",
    "}\n",
    "\n",
    "# Spawn and attach cameras\n",
    "cameras = {}\n",
    "for cam_name, transform in camera_configs.items():\n",
    "    cameras[cam_name] = world.spawn_actor(camera_bp, transform, attach_to=vehicle)\n",
    "world.tick()\n",
    "\n",
    "print(f\"‚úì {len(cameras)} cameras attached (1920x1080, 90¬∞ FOV)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58bdcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute camera intrinsic matrix K for LiDAR projection\n",
    "image_w = int(camera_bp.get_attribute(\"image_size_x\").as_int())\n",
    "image_h = int(camera_bp.get_attribute(\"image_size_y\").as_int())\n",
    "fov = float(camera_bp.get_attribute(\"fov\").as_float())\n",
    "focal = image_w / (2.0 * np.tan(fov * np.pi / 360.0))\n",
    "\n",
    "K = np.identity(3)\n",
    "K[0, 0] = K[1, 1] = focal\n",
    "K[0, 2] = image_w / 2.0\n",
    "K[1, 2] = image_h / 2.0\n",
    "\n",
    "print(f\"‚úì Camera intrinsics computed (focal={focal:.1f}px)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f466d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and attach LiDAR (64 channels, 100m range, 20Hz)\n",
    "lidar_bp = blueprint_library.find('sensor.lidar.ray_cast')\n",
    "lidar_bp.set_attribute('channels', '64')\n",
    "lidar_bp.set_attribute('points_per_second', '1200000')\n",
    "lidar_bp.set_attribute('rotation_frequency', '20')\n",
    "lidar_bp.set_attribute('range', '100')\n",
    "lidar_bp.set_attribute('upper_fov', '10')\n",
    "lidar_bp.set_attribute('lower_fov', '-30')\n",
    "\n",
    "lidar_transform = carla.Transform(carla.Location(x=0.0, z=2.5), carla.Rotation(pitch=0, yaw=0))\n",
    "lidar = world.spawn_actor(lidar_bp, lidar_transform, attach_to=vehicle)\n",
    "world.tick()\n",
    "\n",
    "print(\"‚úì LiDAR attached (64ch, 100m, 20Hz)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02832381",
   "metadata": {},
   "source": [
    "## 4. Spectator View Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup third-person spectator view\n",
    "spectator = world.get_spectator()\n",
    "\n",
    "def update_spectator_view():\n",
    "    \"\"\"Position spectator behind and above vehicle\"\"\"\n",
    "    transform = vehicle.get_transform()\n",
    "    spectator.set_transform(carla.Transform(\n",
    "        transform.location + carla.Location(x=0, z=4),\n",
    "        carla.Rotation(pitch=-15, yaw=transform.rotation.yaw)\n",
    "    ))\n",
    "\n",
    "update_spectator_view()\n",
    "print(\"‚úì Spectator view configured\")\n",
    "world.tick()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1767598",
   "metadata": {},
   "source": [
    "## 5. Sensor Fusion: LiDAR-to-Camera Projection\n",
    "\n",
    "Projects 3D LiDAR points onto 2D camera images using camera intrinsics. Synchronized queues ensure temporal alignment across all sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP OUTPUT DIRECTORIES & SENSOR QUEUES\n",
    "# ============================================================================\n",
    "output_base_dir = \"carla_output\"\n",
    "camera_dirs = {name: os.path.join(output_base_dir, f\"camera_{name}\") for name in cameras.keys()}\n",
    "lidar_dir = os.path.join(output_base_dir, \"lidar\")\n",
    "projection_dir = os.path.join(output_base_dir, \"lidar_projection\")\n",
    "fusion_dir = os.path.join(output_base_dir, \"fusion\")\n",
    "\n",
    "for directory in list(camera_dirs.values()) + [lidar_dir, projection_dir, fusion_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Synchronous sensor queues\n",
    "camera_queues = {cam_name: queue.Queue() for cam_name in cameras.keys()}\n",
    "lidar_queue = queue.Queue()\n",
    "frame_counter = 0\n",
    "\n",
    "# Capture configuration\n",
    "fixed_delta = world.get_settings().fixed_delta_seconds\n",
    "capture_interval_seconds = 0.5\n",
    "capture_interval_ticks = int(capture_interval_seconds / fixed_delta)\n",
    "DOT_EXTENT = 2  # LiDAR projection dot size\n",
    "\n",
    "# Distance weighting parameters for steering\n",
    "CLOSE_DISTANCE_THRESHOLD = 25.0  # Points closer than this get 4x weight (meters)\n",
    "CLOSE_WEIGHT_MULTIPLIER = 4.0    # Weight multiplier for close objects\n",
    "\n",
    "print(f\"‚úì Output directories created: {output_base_dir}/\")\n",
    "print(f\"‚úì Capture interval: {capture_interval_seconds}s ({capture_interval_ticks} ticks)\")\n",
    "print(f\"‚úì Close object emphasis: {CLOSE_WEIGHT_MULTIPLIER}x weight for objects < {CLOSE_DISTANCE_THRESHOLD}m\")\n",
    "\n",
    "# ============================================================================\n",
    "# SENSOR CALLBACKS\n",
    "# ============================================================================\n",
    "def create_image_callback(cam_name):\n",
    "    def process_image(image):\n",
    "        camera_queues[cam_name].put(image)\n",
    "    return process_image\n",
    "\n",
    "def lidar_callback(point_cloud):\n",
    "    lidar_queue.put(point_cloud)\n",
    "\n",
    "# ============================================================================\n",
    "# LIDAR-TO-CAMERA PROJECTION\n",
    "# ============================================================================\n",
    "def project_lidar_to_camera(lidar_data, camera, im_array):\n",
    "    \"\"\"Project 3D LiDAR points onto 2D camera image using intrinsics.\n",
    "    \n",
    "    Returns:\n",
    "        im_array: Image with projected LiDAR points\n",
    "        num_points: Total number of projected points\n",
    "        weighted_density: Distance-weighted density (close objects weighted 4x more)\n",
    "    \"\"\"\n",
    "    # Parse point cloud (x, y, z, intensity)\n",
    "    p_cloud = np.frombuffer(lidar_data.raw_data, dtype=np.dtype('f4')).reshape((-1, 4))\n",
    "    intensity = p_cloud[:, 3]\n",
    "    \n",
    "    # Transform: LiDAR -> World -> Camera\n",
    "    local_points = np.r_[p_cloud[:, :3].T, [np.ones(len(p_cloud))]]\n",
    "    world_points = np.dot(lidar.get_transform().get_matrix(), local_points)\n",
    "    sensor_points = np.dot(np.array(camera.get_transform().get_inverse_matrix()), world_points)\n",
    "    \n",
    "    # Convert to camera coordinates\n",
    "    camera_coords = np.array([sensor_points[1], sensor_points[2] * -1, sensor_points[0]])\n",
    "    \n",
    "    # Project to 2D\n",
    "    points_2d = np.dot(K, camera_coords)\n",
    "    points_2d = np.array([points_2d[0] / points_2d[2], points_2d[1] / points_2d[2], points_2d[2]]).T\n",
    "    \n",
    "    # Filter points in image bounds\n",
    "    mask = (points_2d[:, 0] > 0) & (points_2d[:, 0] < image_w) & \\\n",
    "           (points_2d[:, 1] > 0) & (points_2d[:, 1] < image_h) & \\\n",
    "           (points_2d[:, 2] > 0)\n",
    "    points_2d, intensity = points_2d[mask], intensity[mask]\n",
    "    \n",
    "    # Calculate distance-weighted density\n",
    "    # Close objects (< CLOSE_DISTANCE_THRESHOLD) get CLOSE_WEIGHT_MULTIPLIER weight\n",
    "    depths = points_2d[:, 2]  # Depth in meters\n",
    "    weights = np.where(depths < CLOSE_DISTANCE_THRESHOLD, CLOSE_WEIGHT_MULTIPLIER, 1.0)\n",
    "    weighted_density = np.sum(weights)\n",
    "    \n",
    "    # Colorize by intensity (Viridis colormap)\n",
    "    intensity = np.clip(1.0 - (4 * intensity - 3), 0.0, 1.0)\n",
    "    colors = np.array([\n",
    "        np.interp(intensity, VID_RANGE, VIRIDIS[:, 2]) * 255.0,\n",
    "        np.interp(intensity, VID_RANGE, VIRIDIS[:, 1]) * 255.0,\n",
    "        np.interp(intensity, VID_RANGE, VIRIDIS[:, 0]) * 255.0\n",
    "    ]).astype(np.uint8).T\n",
    "    \n",
    "    # Draw points on image\n",
    "    u, v = points_2d[:, 0].astype(int), points_2d[:, 1].astype(int)\n",
    "    for i in range(len(points_2d)):\n",
    "        v_min, v_max = max(0, v[i] - DOT_EXTENT), min(image_h, v[i] + DOT_EXTENT + 1)\n",
    "        u_min, u_max = max(0, u[i] - DOT_EXTENT), min(image_w, u[i] + DOT_EXTENT + 1)\n",
    "        im_array[v_min:v_max, u_min:u_max] = colors[i]\n",
    "    \n",
    "    return im_array, len(points_2d), weighted_density\n",
    "\n",
    "# ============================================================================\n",
    "# DATA SAVING FUNCTIONS\n",
    "# ============================================================================\n",
    "def save_camera_image(cam_name, image, frame_num):\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8).reshape((image.height, image.width, 4))[:, :, :3]\n",
    "    filename = os.path.join(camera_dirs[cam_name], f\"frame_{frame_num:06d}.jpg\")\n",
    "    cv2.imwrite(filename, array)\n",
    "    return filename, array\n",
    "\n",
    "def save_projected_image(cam_name, im_array, frame_num):\n",
    "    proj_dir = os.path.join(projection_dir, f\"camera_{cam_name}\")\n",
    "    os.makedirs(proj_dir, exist_ok=True)\n",
    "    filename = os.path.join(proj_dir, f\"frame_{frame_num:06d}.jpg\")\n",
    "    cv2.imwrite(filename, im_array)\n",
    "    return filename\n",
    "\n",
    "def save_lidar_data(point_cloud, frame_num):\n",
    "    points = np.frombuffer(point_cloud.raw_data, dtype=np.float32).reshape((-1, 4))\n",
    "    filename = os.path.join(lidar_dir, f\"lidar_{frame_num:06d}.npy\")\n",
    "    np.save(filename, points)\n",
    "    return filename, len(points)\n",
    "\n",
    "def save_fusion_metadata(frame_num, camera_files, projection_files, lidar_file, \n",
    "                         lidar_points, projection_counts, timestamp):\n",
    "    import json\n",
    "    metadata = {\n",
    "        \"frame_id\": frame_num, \"timestamp\": timestamp,\n",
    "        \"cameras\": camera_files, \"lidar_projections\": projection_files,\n",
    "        \"projection_point_counts\": projection_counts,\n",
    "        \"lidar\": {\"file\": lidar_file, \"num_points\": lidar_points}\n",
    "    }\n",
    "    with open(os.path.join(fusion_dir, f\"frame_{frame_num:06d}.json\"), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    return metadata\n",
    "\n",
    "# ============================================================================\n",
    "# SENSOR DATA PROCESSOR\n",
    "# ============================================================================\n",
    "def process_sensor_queues_with_projections(should_save=False, sim_time=0.0):\n",
    "    \"\"\"Process queues and return projection images and weighted counts for AI\"\"\"\n",
    "    global frame_counter\n",
    "    \n",
    "    # Clear queues if not saving\n",
    "    if not should_save:\n",
    "        for q in list(camera_queues.values()) + [lidar_queue]:\n",
    "            while not q.empty(): q.get()\n",
    "        return None, {}, {}\n",
    "    \n",
    "    # Collect synchronized sensor data\n",
    "    camera_images = {}\n",
    "    for cam_name in cameras.keys():\n",
    "        if not camera_queues[cam_name].empty():\n",
    "            camera_images[cam_name] = camera_queues[cam_name].get()\n",
    "            while not camera_queues[cam_name].empty():\n",
    "                camera_queues[cam_name].get()\n",
    "    \n",
    "    lidar_data = lidar_queue.get() if not lidar_queue.empty() else None\n",
    "    while not lidar_queue.empty(): lidar_queue.get()\n",
    "    \n",
    "    # Process and save if all sensors ready\n",
    "    if len(camera_images) == len(cameras) and lidar_data is not None:\n",
    "        current_frame = frame_counter\n",
    "        frame_counter += 1\n",
    "        \n",
    "        # Save camera images\n",
    "        camera_files, camera_arrays = {}, {}\n",
    "        for cam_name, image in camera_images.items():\n",
    "            camera_files[cam_name], camera_arrays[cam_name] = save_camera_image(cam_name, image, current_frame)\n",
    "        \n",
    "        # Save LiDAR\n",
    "        lidar_file, num_points = save_lidar_data(lidar_data, current_frame)\n",
    "        \n",
    "        # Project and save - now returns weighted density for steering\n",
    "        projection_files, projection_counts, projection_images = {}, {}, {}\n",
    "        for cam_name in cameras.keys():\n",
    "            proj_array = camera_arrays[cam_name].copy()\n",
    "            proj_array, num_proj, weighted_density = project_lidar_to_camera(lidar_data, cameras[cam_name], proj_array)\n",
    "            projection_files[cam_name] = save_projected_image(cam_name, proj_array, current_frame)\n",
    "            projection_counts[cam_name] = weighted_density  # Use weighted density for steering\n",
    "            projection_images[cam_name] = proj_array\n",
    "        \n",
    "        # Save metadata\n",
    "        save_fusion_metadata(current_frame, camera_files, projection_files, \n",
    "                           lidar_file, num_points, projection_counts, sim_time)\n",
    "        \n",
    "        if current_frame % 10 == 0:\n",
    "            print(f\"Frame {current_frame}: {num_points} LiDAR pts, weighted density: L={projection_counts['left']:.0f} F={projection_counts['front']:.0f} R={projection_counts['right']:.0f}\")\n",
    "        \n",
    "        return None, projection_images, projection_counts\n",
    "    \n",
    "    return None, {}, {}\n",
    "\n",
    "# Attach sensor listeners\n",
    "for cam_name, cam in cameras.items():\n",
    "    cam.listen(create_image_callback(cam_name))\n",
    "lidar.listen(lidar_callback)\n",
    "\n",
    "print(f\"‚úì All sensors streaming to queues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dee6bc-c7cb-4bdf-a6e6-6b08079a06fb",
   "metadata": {},
   "source": [
    "## 6. LiDAR Density-Based Steering Algorithm\n",
    "\n",
    "Steers the vehicle based on LiDAR projection point density. The algorithm compares obstacle density between left and right cameras and steers proportionally toward the direction with fewer obstacles (lower density)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950f399d-af90-4e37-b50c-afc868efa54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LIDAR DENSITY-BASED STEERING ALGORITHM\n",
    "# ============================================================================\n",
    "# Steers the vehicle based on LiDAR projection point density:\n",
    "# - Goes STRAIGHT when front has lowest density (clearest path ahead)\n",
    "# - Steers toward the direction with LOWER density (fewer obstacles)\n",
    "# - Proportional steering based on density difference ratio\n",
    "#\n",
    "# Input:  projection_counts (dict) - Number of LiDAR points projected per camera\n",
    "#         - projection_counts['front'] : int - Points visible in front camera\n",
    "#         - projection_counts['left']  : int - Points visible in left camera\n",
    "#         - projection_counts['right'] : int - Points visible in right camera\n",
    "#\n",
    "# Output: carla.VehicleControl object with throttle and proportional steering\n",
    "# ============================================================================\n",
    "\n",
    "# Tunable parameters\n",
    "THROTTLE = 0.3          # Constant forward throttle (0.0 to 1.0)\n",
    "STEERING_GAIN = 0.8     # Steering sensitivity multiplier (higher = more aggressive)\n",
    "\n",
    "def lidar_density_steering(projection_counts):\n",
    "    \"\"\"\n",
    "    LiDAR density-based steering algorithm.\n",
    "    \n",
    "    If front has the lowest density (clearest path), go straight.\n",
    "    Otherwise, steer toward the direction with fewer LiDAR points.\n",
    "    \n",
    "    Args:\n",
    "        projection_counts: dict with 'front', 'left', 'right' LiDAR point counts\n",
    "        \n",
    "    Returns:\n",
    "        carla.VehicleControl: Control commands with proportional steering\n",
    "    \"\"\"\n",
    "    # Get point counts for each camera view\n",
    "    front_count = projection_counts.get('front', 0)\n",
    "    left_count = projection_counts.get('left', 0)\n",
    "    right_count = projection_counts.get('right', 0)\n",
    "    \n",
    "    # If front has the lowest density (fewest obstacles), go straight\n",
    "    if front_count < left_count and front_count < right_count:\n",
    "        steer = 0.0\n",
    "    else:\n",
    "        # Calculate proportional steering based on density difference\n",
    "        # Positive steer = turn right, Negative steer = turn left\n",
    "        # We steer TOWARD lower density (away from obstacles)\n",
    "        total_side = left_count + right_count\n",
    "        \n",
    "        if total_side > 0:\n",
    "            # Normalized difference: positive when left has more points (steer right)\n",
    "            # negative when right has more points (steer left)\n",
    "            density_diff = (left_count - right_count) / total_side\n",
    "            steer = density_diff * STEERING_GAIN\n",
    "        else:\n",
    "            # No side obstacles detected, go straight\n",
    "            steer = 0.0\n",
    "    \n",
    "    # Clamp steering to valid range\n",
    "    steer = max(-1.0, min(1.0, steer))\n",
    "    \n",
    "    # Create control command\n",
    "    control = carla.VehicleControl()\n",
    "    control.throttle = THROTTLE\n",
    "    control.steer = steer\n",
    "    control.brake = 0.0\n",
    "    \n",
    "    return control\n",
    "\n",
    "print(f\"‚úì LiDAR density steering algorithm ready\")\n",
    "print(f\"  Throttle: {THROTTLE}, Steering gain: {STEERING_GAIN}\")\n",
    "print(f\"  Rule: Go STRAIGHT when front has lowest density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b28904",
   "metadata": {},
   "source": [
    "## 7. Run Simulation (Density-Based Steering)\n",
    "\n",
    "Vehicle drives using the LiDAR density-based steering algorithm. Each tick, LiDAR projection counts are used to calculate proportional steering toward lower obstacle density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable autopilot - using custom density-based steering\n",
    "vehicle.set_autopilot(False)\n",
    "simulation_duration = 30\n",
    "total_ticks = int(simulation_duration / fixed_delta)\n",
    "\n",
    "print(f\"‚úì Density-based steering enabled for {simulation_duration}s ({total_ticks} ticks)\")\n",
    "print(\"Watch CARLA window for autonomous driving\\n\")\n",
    "\n",
    "tick_count = 0\n",
    "last_print_time = 0\n",
    "current_projection_counts = {}\n",
    "\n",
    "try:\n",
    "    while tick_count < total_ticks:\n",
    "        world.tick()\n",
    "        tick_count += 1\n",
    "        \n",
    "        # Wait for synchronized sensor data (timeout 2s)\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < 2.0:\n",
    "            if all(camera_queues[c].qsize() > 0 for c in cameras.keys()) and lidar_queue.qsize() > 0:\n",
    "                break\n",
    "            time.sleep(0.001)\n",
    "        \n",
    "        # Process sensor data\n",
    "        should_save = (tick_count % capture_interval_ticks == 0)\n",
    "        _, projection_images, projection_counts = process_sensor_queues_with_projections(should_save, tick_count * fixed_delta)\n",
    "        \n",
    "        # Update current projection counts when new data available\n",
    "        if projection_counts:\n",
    "            current_projection_counts = projection_counts\n",
    "        \n",
    "        # Apply density-based steering control\n",
    "        if current_projection_counts:\n",
    "            control = lidar_density_steering(current_projection_counts)\n",
    "            vehicle.apply_control(control)\n",
    "        \n",
    "        # Update spectator view\n",
    "        update_spectator_view()\n",
    "        \n",
    "        # Print status every 5s\n",
    "        current_sim_time = tick_count * fixed_delta\n",
    "        if int(current_sim_time) % 5 == 0 and int(current_sim_time) != last_print_time:\n",
    "            last_print_time = int(current_sim_time)\n",
    "            loc = vehicle.get_location()\n",
    "            vel = vehicle.get_velocity()\n",
    "            speed = 3.6 * np.sqrt(vel.x**2 + vel.y**2 + vel.z**2)\n",
    "            \n",
    "            # Show density info with updated steering logic\n",
    "            l = current_projection_counts.get('left', 0)\n",
    "            f = current_projection_counts.get('front', 0)\n",
    "            r = current_projection_counts.get('right', 0)\n",
    "            \n",
    "            # Determine steering direction (matches algorithm logic)\n",
    "            if f < l and f < r:\n",
    "                steer_dir = \"STRAIGHT (front clear)\"\n",
    "            elif l > r:\n",
    "                steer_dir = \"RIGHT\"\n",
    "            elif r > l:\n",
    "                steer_dir = \"LEFT\"\n",
    "            else:\n",
    "                steer_dir = \"STRAIGHT\"\n",
    "            \n",
    "            print(f\"[{int(current_sim_time)}s] Speed: {speed:.1f} km/h | Density L:{l:.0f} F:{f:.0f} R:{r:.0f} ‚Üí {steer_dir}\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚úó Simulation interrupted by user\")\n",
    "\n",
    "print(\"\\n‚úì Simulation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc65af",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "Stop sensors, destroy actors, and restore asynchronous mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ab1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore async mode\n",
    "world.apply_settings(original_settings)\n",
    "traffic_manager.set_synchronous_mode(False)\n",
    "\n",
    "# Stop sensor listeners\n",
    "for cam in cameras.values():\n",
    "    cam.stop()\n",
    "lidar.stop()\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Destroy actors\n",
    "for cam in cameras.values():\n",
    "    cam.destroy()\n",
    "lidar.destroy()\n",
    "vehicle.destroy()\n",
    "world.tick()\n",
    "\n",
    "print(\"‚úì Cleanup completed\")\n",
    "print(f\"\\nüìÅ Data saved in: {output_base_dir}/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ camera_front/left/right/  (raw images)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ lidar/                     (point clouds)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ lidar_projection/          (projected images)\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ fusion/                    (JSON metadata)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
