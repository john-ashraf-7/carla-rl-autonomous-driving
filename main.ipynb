{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0f4574",
   "metadata": {},
   "source": [
    "# CARLA Client - Clean Environment Setup with Autopilot Car (Synchronous Mode)\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. Connecting to CARLA simulator in **synchronous mode**\n",
    "2. Removing all unnecessary objects from the map (keeping only road, car, pavement)\n",
    "3. Spawning a vehicle with a camera\n",
    "4. Enabling autopilot mode\n",
    "5. Setting up third-person spectator view\n",
    "6. Deterministic sensor data capture using queues\n",
    "7. Cleanup at the end (restoring async mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fc85f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARLA library imported successfully!\n",
      "Viridis colormap loaded for LiDAR projection visualization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32379/3351144801.py:11: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  VIRIDIS = np.array(cm.get_cmap('viridis').colors)\n"
     ]
    }
   ],
   "source": [
    "import carla\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import queue\n",
    "from matplotlib import cm\n",
    "\n",
    "# Create VIRIDIS colormap for LiDAR intensity visualization\n",
    "VIRIDIS = np.array(cm.get_cmap('viridis').colors)\n",
    "VID_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "\n",
    "print(\"CARLA library imported successfully!\")\n",
    "print(\"Viridis colormap loaded for LiDAR projection visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a9a82",
   "metadata": {},
   "source": [
    "## Step 1: Connect to CARLA Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e54b9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the CARLA server\n",
    "client = carla.Client('localhost', 2000)\n",
    "client.set_timeout(10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb1fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to CARLA server!\n",
      "Map: Carla/Maps/Town10HD_Opt\n",
      "Synchronous mode enabled with fixed_delta_seconds = 0.05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the world\n",
    "world = client.get_world()\n",
    "\n",
    "# Get the map\n",
    "carla_map = world.get_map()\n",
    "\n",
    "# Store original settings to restore later\n",
    "original_settings = world.get_settings()\n",
    "\n",
    "# Enable synchronous mode\n",
    "settings = world.get_settings()\n",
    "settings.synchronous_mode = True\n",
    "settings.fixed_delta_seconds = 0.05  # 20 FPS (50ms per tick)\n",
    "world.apply_settings(settings)\n",
    "\n",
    "# Set Traffic Manager to synchronous mode as well\n",
    "traffic_manager = client.get_trafficmanager(8000)\n",
    "traffic_manager.set_synchronous_mode(True)\n",
    "\n",
    "print(f\"Connected to CARLA server!\")\n",
    "print(f\"Map: {carla_map.name}\")\n",
    "print(f\"Synchronous mode enabled with fixed_delta_seconds = {settings.fixed_delta_seconds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e7080",
   "metadata": {},
   "source": [
    "## Step 2: Remove All Static Environment Objects\n",
    "\n",
    "We'll remove all static environment objects that load with the map. This includes:\n",
    "- Buildings\n",
    "- Props (trees, traffic signs, street lights, etc.)\n",
    "- Static meshes\n",
    "- Keeping only the road surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd5a725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2312"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.unload_map_layer(carla.MapLayer.Buildings)\n",
    "world.unload_map_layer(carla.MapLayer.Decals)\n",
    "world.unload_map_layer(carla.MapLayer.Foliage)\n",
    "world.unload_map_layer(carla.MapLayer.ParkedVehicles)\n",
    "world.unload_map_layer(carla.MapLayer.Particles)\n",
    "world.unload_map_layer(carla.MapLayer.Props)\n",
    "world.unload_map_layer(carla.MapLayer.Walls)\n",
    "world.tick()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ac8bb",
   "metadata": {},
   "source": [
    "## Step 3: Spawn a Vehicle with Multiple Cameras and LiDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49062329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spawned vehicle: vehicle.tesla.model3\n",
      "Vehicle location: Location(x=0.000000, y=0.000000, z=0.000000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2313"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the blueprint library\n",
    "blueprint_library = world.get_blueprint_library()\n",
    "\n",
    "# Choose a vehicle blueprint (Tesla Model 3)\n",
    "vehicle_bp = blueprint_library.filter('vehicle.tesla.model3')[0]\n",
    "\n",
    "# Get a spawn point\n",
    "spawn_points = carla_map.get_spawn_points()\n",
    "spawn_point = random.choice(spawn_points)\n",
    "\n",
    "# Spawn the vehicle\n",
    "vehicle = world.spawn_actor(vehicle_bp, spawn_point)\n",
    "\n",
    "print(f\"Spawned vehicle: {vehicle.type_id}\")\n",
    "print(f\"Vehicle location: {vehicle.get_location()}\")\n",
    "world.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b56c8c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera 'front' attached to vehicle at Location(x=2.500000, y=0.000000, z=1.000000)\n",
      "Camera 'left' attached to vehicle at Location(x=0.000000, y=-1.000000, z=1.000000)\n",
      "Camera 'right' attached to vehicle at Location(x=0.000000, y=1.000000, z=1.000000)\n",
      "\n",
      "Total cameras attached: 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2314"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get camera blueprint\n",
    "camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "\n",
    "# Set camera attributes\n",
    "camera_bp.set_attribute('image_size_x', '1920')\n",
    "camera_bp.set_attribute('image_size_y', '1080')\n",
    "camera_bp.set_attribute('fov', '90')\n",
    "\n",
    "# Define camera configurations: (name, location, rotation)\n",
    "camera_configs = {\n",
    "    'front': {\n",
    "        'transform': carla.Transform(\n",
    "            carla.Location(x=2.5, z=1.0),  # Front of vehicle\n",
    "            carla.Rotation(pitch=0, yaw=0)  # Looking forward\n",
    "        )\n",
    "    },\n",
    "    'left': {\n",
    "        'transform': carla.Transform(\n",
    "            carla.Location(x=0.0, y=-1.0, z=1.0),  # Left side\n",
    "            carla.Rotation(pitch=0, yaw=-90)  # Looking left\n",
    "        )\n",
    "    },\n",
    "    'right': {\n",
    "        'transform': carla.Transform(\n",
    "            carla.Location(x=0.0, y=1.0, z=1.0),  # Right side\n",
    "            carla.Rotation(pitch=0, yaw=90)  # Looking right\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "# Spawn all cameras\n",
    "cameras = {}\n",
    "for cam_name, config in camera_configs.items():\n",
    "    cam = world.spawn_actor(camera_bp, config['transform'], attach_to=vehicle)\n",
    "    cameras[cam_name] = cam\n",
    "    print(f\"Camera '{cam_name}' attached to vehicle at {config['transform'].location}\")\n",
    "\n",
    "print(f\"\\nTotal cameras attached: {len(cameras)}\")\n",
    "world.tick()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f58bdcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera intrinsic matrix K computed:\n",
      "  Image size: 1920 x 1080\n",
      "  FOV: 90.0°\n",
      "  Focal length: 960.00 pixels\n",
      "K =\n",
      "[[960.   0. 960.]\n",
      " [  0. 960. 540.]\n",
      " [  0.   0.   1.]]\n"
     ]
    }
   ],
   "source": [
    "# Build camera intrinsic matrices (K) for LiDAR projection\n",
    "# K = [[Fx,  0, image_w/2],\n",
    "#      [ 0, Fy, image_h/2],\n",
    "#      [ 0,  0,         1]]\n",
    "\n",
    "image_w = int(camera_bp.get_attribute(\"image_size_x\").as_int())\n",
    "image_h = int(camera_bp.get_attribute(\"image_size_y\").as_int())\n",
    "fov = float(camera_bp.get_attribute(\"fov\").as_float())\n",
    "\n",
    "# Calculate focal length from FOV\n",
    "# fov = 2 * atan(image_w / (2 * focal))\n",
    "# focal = image_w / (2 * tan(fov/2))\n",
    "focal = image_w / (2.0 * np.tan(fov * np.pi / 360.0))\n",
    "\n",
    "# Since pixel aspect ratio is 1, Fx = Fy\n",
    "K = np.identity(3)\n",
    "K[0, 0] = K[1, 1] = focal\n",
    "K[0, 2] = image_w / 2.0\n",
    "K[1, 2] = image_h / 2.0\n",
    "\n",
    "print(f\"Camera intrinsic matrix K computed:\")\n",
    "print(f\"  Image size: {image_w} x {image_h}\")\n",
    "print(f\"  FOV: {fov}°\")\n",
    "print(f\"  Focal length: {focal:.2f} pixels\")\n",
    "print(f\"K =\\n{K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f466d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiDAR attached to vehicle at Location(x=0.000000, y=0.000000, z=2.500000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2318"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get LiDAR blueprint\n",
    "lidar_bp = blueprint_library.find('sensor.lidar.ray_cast')\n",
    "\n",
    "# Set LiDAR attributes\n",
    "lidar_bp.set_attribute('channels', '64')\n",
    "lidar_bp.set_attribute('points_per_second', '1200000')\n",
    "lidar_bp.set_attribute('rotation_frequency', '20')\n",
    "lidar_bp.set_attribute('range', '100')\n",
    "lidar_bp.set_attribute('upper_fov', '10')\n",
    "lidar_bp.set_attribute('lower_fov', '-30')\n",
    "\n",
    "# Create transform for LiDAR (mounted on top of the vehicle)\n",
    "lidar_transform = carla.Transform(\n",
    "    carla.Location(x=0.0, z=2.5),  # On top of the vehicle\n",
    "    carla.Rotation(pitch=0, yaw=0)\n",
    ")\n",
    "\n",
    "# Spawn the LiDAR attached to the vehicle\n",
    "lidar = world.spawn_actor(lidar_bp, lidar_transform, attach_to=vehicle)\n",
    "\n",
    "print(f\"LiDAR attached to vehicle at {lidar_transform.location}\")\n",
    "world.tick()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ecd7a",
   "metadata": {},
   "source": [
    "## Step 4: Enable Autopilot Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96f6fd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autopilot enabled!\n",
      "Vehicle will now drive automatically\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2319"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enable autopilot for the vehicle\n",
    "vehicle.set_autopilot(True)\n",
    "\n",
    "print(\"Autopilot enabled!\")\n",
    "print(\"Vehicle will now drive automatically\")\n",
    "world.tick()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02832381",
   "metadata": {},
   "source": [
    "## Step 5: Set Up Third-Person Spectator View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e23e631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third-person spectator view set!\n",
      "The spectator camera is following the vehicle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2320"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the spectator\n",
    "spectator = world.get_spectator()\n",
    "\n",
    "# Function to update spectator position to follow the vehicle\n",
    "def update_spectator_view():\n",
    "    # Get vehicle transform\n",
    "    vehicle_transform = vehicle.get_transform()\n",
    "    \n",
    "    # Calculate spectator position (behind and above the vehicle)\n",
    "    spectator_transform = carla.Transform(\n",
    "        vehicle_transform.location + carla.Location(x=0, z=4),\n",
    "        carla.Rotation(pitch=-15, yaw=vehicle_transform.rotation.yaw)\n",
    "    )\n",
    "    \n",
    "    # Set spectator transform\n",
    "    spectator.set_transform(spectator_transform)\n",
    "\n",
    "# Initial spectator position\n",
    "update_spectator_view()\n",
    "\n",
    "print(\"Third-person spectator view set!\")\n",
    "print(\"The spectator camera is following the vehicle\")\n",
    "world.tick()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1767598",
   "metadata": {},
   "source": [
    "## Step 6: Set Up Sensor Recording with LiDAR-to-Camera Projection (Synchronous Mode)\n",
    "\n",
    "This sets up recording from all 3 cameras (front, left, right) and the LiDAR sensor with **LiDAR-to-Camera Projection**:\n",
    "- All sensors from the same simulation tick share the **same frame number**\n",
    "- **LiDAR points are projected onto each camera image** using camera intrinsics and transforms\n",
    "- Projected points are colored by intensity using the Viridis colormap\n",
    "- Fusion metadata JSON files link camera images + LiDAR for each frame\n",
    "- Ensures temporal synchronization for downstream fusion algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "545b87b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directory: carla_output/camera_front\n",
      "Created output directory: carla_output/camera_left\n",
      "Created output directory: carla_output/camera_right\n",
      "Created output directory: carla_output/lidar\n",
      "Created output directory: carla_output/fusion\n",
      "Created output directory: carla_output/lidar_projection\n",
      "Camera 'front' listener attached\n",
      "Camera 'left' listener attached\n",
      "Camera 'right' listener attached\n",
      "LiDAR listener attached\n",
      "\n",
      "All sensors ready for synchronous recording with LIDAR-TO-CAMERA PROJECTION\n",
      "Capture interval: every 10 ticks (0.5s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create output directories\n",
    "output_base_dir = \"carla_output\"\n",
    "camera_dirs = {}\n",
    "for cam_name in cameras.keys():\n",
    "    cam_dir = os.path.join(output_base_dir, f\"camera_{cam_name}\")\n",
    "    os.makedirs(cam_dir, exist_ok=True)\n",
    "    camera_dirs[cam_name] = cam_dir\n",
    "    print(f\"Created output directory: {cam_dir}\")\n",
    "\n",
    "lidar_dir = os.path.join(output_base_dir, \"lidar\")\n",
    "os.makedirs(lidar_dir, exist_ok=True)\n",
    "print(f\"Created output directory: {lidar_dir}\")\n",
    "\n",
    "fusion_dir = os.path.join(output_base_dir, \"fusion\")\n",
    "os.makedirs(fusion_dir, exist_ok=True)\n",
    "print(f\"Created output directory: {fusion_dir}\")\n",
    "\n",
    "projection_dir = os.path.join(output_base_dir, \"lidar_projection\")\n",
    "os.makedirs(projection_dir, exist_ok=True)\n",
    "print(f\"Created output directory: {projection_dir}\")\n",
    "\n",
    "# Queues for synchronous sensor data collection\n",
    "camera_queues = {cam_name: queue.Queue() for cam_name in cameras.keys()}\n",
    "lidar_queue = queue.Queue()\n",
    "\n",
    "# Unified frame counter\n",
    "frame_counter = 0\n",
    "\n",
    "# Capture interval\n",
    "fixed_delta = world.get_settings().fixed_delta_seconds\n",
    "capture_interval_seconds = 0.5\n",
    "capture_interval_ticks = int(capture_interval_seconds / fixed_delta)\n",
    "\n",
    "# Projection settings\n",
    "DOT_EXTENT = 2\n",
    "\n",
    "def create_image_callback(cam_name):\n",
    "    \"\"\"Create a callback function for each camera\"\"\"\n",
    "    def process_image(image):\n",
    "        camera_queues[cam_name].put(image)\n",
    "    return process_image\n",
    "\n",
    "def lidar_callback(point_cloud):\n",
    "    \"\"\"Callback that puts LiDAR data in a queue\"\"\"\n",
    "    lidar_queue.put(point_cloud)\n",
    "\n",
    "def project_lidar_to_camera(lidar_data, camera, im_array):\n",
    "    \"\"\"Project LiDAR points onto camera image\"\"\"\n",
    "    # Get point cloud data\n",
    "    p_cloud_size = len(lidar_data)\n",
    "    p_cloud = np.copy(np.frombuffer(lidar_data.raw_data, dtype=np.dtype('f4')))\n",
    "    p_cloud = np.reshape(p_cloud, (p_cloud_size, 4))\n",
    "    \n",
    "    # Extract intensity\n",
    "    intensity = np.array(p_cloud[:, 3])\n",
    "    \n",
    "    # Point cloud in lidar sensor space\n",
    "    local_lidar_points = np.array(p_cloud[:, :3]).T\n",
    "    local_lidar_points = np.r_[local_lidar_points, [np.ones(local_lidar_points.shape[1])]]\n",
    "    \n",
    "    # Transform to world space\n",
    "    lidar_2_world = lidar.get_transform().get_matrix()\n",
    "    world_points = np.dot(lidar_2_world, local_lidar_points)\n",
    "    \n",
    "    # Transform to camera space\n",
    "    world_2_camera = np.array(camera.get_transform().get_inverse_matrix())\n",
    "    sensor_points = np.dot(world_2_camera, world_points)\n",
    "    \n",
    "    # Convert coordinate system\n",
    "    point_in_camera_coords = np.array([\n",
    "        sensor_points[1],\n",
    "        sensor_points[2] * -1,\n",
    "        sensor_points[0]])\n",
    "    \n",
    "    # Project to 2D\n",
    "    points_2d = np.dot(K, point_in_camera_coords)\n",
    "    points_2d = np.array([\n",
    "        points_2d[0, :] / points_2d[2, :],\n",
    "        points_2d[1, :] / points_2d[2, :],\n",
    "        points_2d[2, :]])\n",
    "    \n",
    "    # Filter points\n",
    "    points_2d = points_2d.T\n",
    "    intensity = intensity.T\n",
    "    \n",
    "    points_in_canvas_mask = \\\n",
    "        (points_2d[:, 0] > 0.0) & (points_2d[:, 0] < image_w) & \\\n",
    "        (points_2d[:, 1] > 0.0) & (points_2d[:, 1] < image_h) & \\\n",
    "        (points_2d[:, 2] > 0.0)\n",
    "    \n",
    "    points_2d = points_2d[points_in_canvas_mask]\n",
    "    intensity = intensity[points_in_canvas_mask]\n",
    "    \n",
    "    # Get coordinates\n",
    "    u_coord = points_2d[:, 0].astype(int)\n",
    "    v_coord = points_2d[:, 1].astype(int)\n",
    "    \n",
    "    # Normalize intensity\n",
    "    intensity = 4 * intensity - 3\n",
    "    intensity = np.clip(intensity, 0.0, 1.0)\n",
    "    intensity = 1.0 - intensity\n",
    "    \n",
    "    # Map to colors\n",
    "    color_map = np.array([\n",
    "        np.interp(intensity, VID_RANGE, VIRIDIS[:, 2]) * 255.0,\n",
    "        np.interp(intensity, VID_RANGE, VIRIDIS[:, 1]) * 255.0,\n",
    "        np.interp(intensity, VID_RANGE, VIRIDIS[:, 0]) * 255.0\n",
    "    ]).astype(np.uint8).T\n",
    "    \n",
    "    # Draw points\n",
    "    if DOT_EXTENT <= 0:\n",
    "        im_array[v_coord, u_coord] = color_map\n",
    "    else:\n",
    "        for i in range(len(points_2d)):\n",
    "            v_min = max(0, v_coord[i] - DOT_EXTENT)\n",
    "            v_max = min(image_h, v_coord[i] + DOT_EXTENT + 1)\n",
    "            u_min = max(0, u_coord[i] - DOT_EXTENT)\n",
    "            u_max = min(image_w, u_coord[i] + DOT_EXTENT + 1)\n",
    "            im_array[v_min:v_max, u_min:u_max] = color_map[i]\n",
    "    \n",
    "    return im_array, len(points_2d)\n",
    "\n",
    "def save_camera_image(cam_name, image, frame_num):\n",
    "    \"\"\"Save camera image to disk\"\"\"\n",
    "    array = np.copy(np.frombuffer(image.raw_data, dtype=np.dtype(\"uint8\")))\n",
    "    array = np.reshape(array, (image.height, image.width, 4))\n",
    "    array = array[:, :, :3]\n",
    "    \n",
    "    filename = os.path.join(camera_dirs[cam_name], f\"frame_{frame_num:06d}.jpg\")\n",
    "    cv2.imwrite(filename, array)\n",
    "    \n",
    "    return filename, array\n",
    "\n",
    "def save_projected_image(cam_name, im_array, frame_num):\n",
    "    \"\"\"Save camera image with LiDAR projection overlay\"\"\"\n",
    "    proj_cam_dir = os.path.join(projection_dir, f\"camera_{cam_name}\")\n",
    "    os.makedirs(proj_cam_dir, exist_ok=True)\n",
    "    \n",
    "    filename = os.path.join(proj_cam_dir, f\"frame_{frame_num:06d}.jpg\")\n",
    "    cv2.imwrite(filename, im_array)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def save_lidar_data(point_cloud, frame_num):\n",
    "    \"\"\"Save LiDAR point cloud to disk\"\"\"\n",
    "    points = np.frombuffer(point_cloud.raw_data, dtype=np.dtype('f4'))\n",
    "    points = np.reshape(points, (-1, 4))\n",
    "    \n",
    "    filename = os.path.join(lidar_dir, f\"lidar_{frame_num:06d}.npy\")\n",
    "    np.save(filename, points)\n",
    "    \n",
    "    return filename, len(points)\n",
    "\n",
    "def save_fusion_metadata(frame_num, camera_files, projection_files, lidar_file, lidar_points, projection_counts, timestamp):\n",
    "    \"\"\"Save metadata associating camera images with LiDAR scan\"\"\"\n",
    "    import json\n",
    "    \n",
    "    metadata = {\n",
    "        \"frame_id\": frame_num,\n",
    "        \"timestamp\": timestamp,\n",
    "        \"cameras\": camera_files,\n",
    "        \"lidar_projections\": projection_files,\n",
    "        \"projection_point_counts\": projection_counts,\n",
    "        \"lidar\": {\n",
    "            \"file\": lidar_file,\n",
    "            \"num_points\": lidar_points\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_file = os.path.join(fusion_dir, f\"frame_{frame_num:06d}.json\")\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def process_sensor_queues_with_projections(should_save=False, sim_time=0.0):\n",
    "    \"\"\"Process sensor queues and return projection images for AI controller\"\"\"\n",
    "    global frame_counter\n",
    "    \n",
    "    if not should_save:\n",
    "        for cam_name in cameras.keys():\n",
    "            while not camera_queues[cam_name].empty():\n",
    "                camera_queues[cam_name].get()\n",
    "        while not lidar_queue.empty():\n",
    "            lidar_queue.get()\n",
    "        return None, {}\n",
    "    \n",
    "    # Collect sensor data\n",
    "    camera_images = {}\n",
    "    lidar_data = None\n",
    "    \n",
    "    for cam_name in cameras.keys():\n",
    "        if not camera_queues[cam_name].empty():\n",
    "            camera_images[cam_name] = camera_queues[cam_name].get()\n",
    "            while not camera_queues[cam_name].empty():\n",
    "                camera_queues[cam_name].get()\n",
    "    \n",
    "    if not lidar_queue.empty():\n",
    "        lidar_data = lidar_queue.get()\n",
    "        while not lidar_queue.empty():\n",
    "            lidar_queue.get()\n",
    "    \n",
    "    projection_images = {}\n",
    "    \n",
    "    if len(camera_images) == len(cameras) and lidar_data is not None:\n",
    "        current_frame = frame_counter\n",
    "        frame_counter += 1\n",
    "        \n",
    "        # Save camera images\n",
    "        camera_files = {}\n",
    "        camera_arrays = {}\n",
    "        for cam_name, image in camera_images.items():\n",
    "            filename, array = save_camera_image(cam_name, image, current_frame)\n",
    "            camera_files[cam_name] = filename\n",
    "            camera_arrays[cam_name] = array\n",
    "        \n",
    "        # Save LiDAR data\n",
    "        lidar_file, num_points = save_lidar_data(lidar_data, current_frame)\n",
    "        \n",
    "        # Project LiDAR onto cameras\n",
    "        projection_files = {}\n",
    "        projection_counts = {}\n",
    "        for cam_name in cameras.keys():\n",
    "            proj_array = camera_arrays[cam_name].copy()\n",
    "            proj_array, num_projected = project_lidar_to_camera(\n",
    "                lidar_data, cameras[cam_name], proj_array)\n",
    "            \n",
    "            proj_filename = save_projected_image(cam_name, proj_array, current_frame)\n",
    "            projection_files[cam_name] = proj_filename\n",
    "            projection_counts[cam_name] = num_projected\n",
    "            projection_images[cam_name] = proj_array\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = save_fusion_metadata(\n",
    "            current_frame, camera_files, projection_files,\n",
    "            lidar_file, num_points, projection_counts, sim_time\n",
    "        )\n",
    "        \n",
    "        if current_frame % 10 == 0:\n",
    "            total_projected = sum(projection_counts.values())\n",
    "            print(f\"Frame {current_frame}: Saved 3 cameras + LiDAR ({num_points} points) + Projections ({total_projected} projected)\")\n",
    "        \n",
    "        return metadata, projection_images\n",
    "    \n",
    "    return None, {}\n",
    "\n",
    "# Attach callbacks\n",
    "for cam_name, cam in cameras.items():\n",
    "    cam.listen(create_image_callback(cam_name))\n",
    "    print(f\"Camera '{cam_name}' listener attached\")\n",
    "\n",
    "lidar.listen(lidar_callback)\n",
    "print(f\"LiDAR listener attached\")\n",
    "\n",
    "print(f\"\\nAll sensors ready for synchronous recording with LIDAR-TO-CAMERA PROJECTION\")\n",
    "print(f\"Capture interval: every {capture_interval_ticks} ticks ({capture_interval_seconds}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dee6bc-c7cb-4bdf-a6e6-6b08079a06fb",
   "metadata": {},
   "source": [
    "## AI Trajectory Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "950f399d-af90-4e37-b50c-afc868efa54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "AI TRAJECTORY CONTROLLER READY\n",
      "============================================================\n",
      "Vehicle will be controlled by AI based on LiDAR projections\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# AI controller state\n",
    "ai_steering_history = []\n",
    "ai_max_history = 5\n",
    "ai_default_throttle = 0.5\n",
    "ai_max_steering = 1.0\n",
    "\n",
    "def analyze_lidar_projection(projection_image):\n",
    "    \"\"\"Analyze LiDAR projection to detect obstacles\"\"\"\n",
    "    gray = cv2.cvtColor(projection_image, cv2.COLOR_BGR2GRAY)\n",
    "    lidar_mask = gray > 10\n",
    "    \n",
    "    h, w = lidar_mask.shape\n",
    "    zone_width = w // 3\n",
    "    \n",
    "    # Count points in zones\n",
    "    left_zone = lidar_mask[:, :zone_width]\n",
    "    center_zone = lidar_mask[:, zone_width:2*zone_width]\n",
    "    right_zone = lidar_mask[:, 2*zone_width:]\n",
    "    \n",
    "    left_density = np.sum(left_zone) / (h * zone_width)\n",
    "    center_density = np.sum(center_zone) / (h * zone_width)\n",
    "    right_density = np.sum(right_zone) / (h * zone_width)\n",
    "    \n",
    "    # Focus on bottom half (closer objects)\n",
    "    bottom_half = lidar_mask[h//2:, :]\n",
    "    bottom_left = bottom_half[:, :zone_width]\n",
    "    bottom_center = bottom_half[:, zone_width:2*zone_width]\n",
    "    bottom_right = bottom_half[:, 2*zone_width:]\n",
    "    \n",
    "    close_left_density = np.sum(bottom_left) / (h//2 * zone_width)\n",
    "    close_center_density = np.sum(bottom_center) / (h//2 * zone_width)\n",
    "    close_right_density = np.sum(bottom_right) / (h//2 * zone_width)\n",
    "    \n",
    "    return {\n",
    "        'left_density': left_density,\n",
    "        'center_density': center_density,\n",
    "        'right_density': right_density,\n",
    "        'close_left_density': close_left_density,\n",
    "        'close_center_density': close_center_density,\n",
    "        'close_right_density': close_right_density,\n",
    "        'total_points': np.sum(lidar_mask)\n",
    "    }\n",
    "\n",
    "def compute_steering(analysis):\n",
    "    \"\"\"Compute steering angle - go straight, steer only when obstacle ahead\"\"\"\n",
    "    global ai_steering_history\n",
    "    \n",
    "    close_center = analysis['close_center_density']\n",
    "    close_left = analysis['close_left_density']\n",
    "    close_right = analysis['close_right_density']\n",
    "    \n",
    "    # Threshold for detecting obstacle in front (increased to reduce false positives)\n",
    "    obstacle_threshold = 0.15\n",
    "    \n",
    "    steering = 0.0\n",
    "    \n",
    "    # Only steer if there's an obstacle directly ahead\n",
    "    if close_center > obstacle_threshold:\n",
    "        # Object in front! Steer toward the clearer side\n",
    "        # Add a small margin to prefer going straight when sides are similar\n",
    "        if close_left < close_right - 0.02:\n",
    "            # Left is clearly clearer, steer left\n",
    "            steering = -0.5\n",
    "        elif close_right < close_left - 0.02:\n",
    "            # Right is clearly clearer, steer right\n",
    "            steering = 0.5\n",
    "        else:\n",
    "            # Both sides similar, prefer slight left to avoid right bias\n",
    "            steering = -0.3\n",
    "    else:\n",
    "        # No obstacle ahead - go straight\n",
    "        steering = 0.0\n",
    "    \n",
    "    # Smooth steering with history\n",
    "    ai_steering_history.append(steering)\n",
    "    if len(ai_steering_history) > ai_max_history:\n",
    "        ai_steering_history.pop(0)\n",
    "    \n",
    "    smoothed_steering = np.mean(ai_steering_history)\n",
    "    \n",
    "    return np.clip(smoothed_steering, -ai_max_steering, ai_max_steering)\n",
    "\n",
    "def compute_throttle_brake(analysis):\n",
    "    \"\"\"Always move forward, slow down only for very close obstacles\"\"\"\n",
    "    close_center = analysis['close_center_density']\n",
    "    \n",
    "    # Very high threshold - only slow down for very close obstacles\n",
    "    danger_threshold = 0.25\n",
    "    \n",
    "    if close_center > danger_threshold:\n",
    "        # Very close obstacle - slow down but keep moving\n",
    "        throttle = 0.3\n",
    "        brake = 0.0\n",
    "    else:\n",
    "        # Normal forward motion\n",
    "        throttle = ai_default_throttle\n",
    "        brake = 0.0\n",
    "    \n",
    "    return throttle, brake\n",
    "\n",
    "def get_ai_control(lidar_projection_images):\n",
    "    \"\"\"Main AI control function - returns vehicle control based on LiDAR projections\"\"\"\n",
    "    front_projection = lidar_projection_images.get('front')\n",
    "    \n",
    "    if front_projection is None:\n",
    "        control = carla.VehicleControl()\n",
    "        control.throttle = 0.0\n",
    "        control.brake = 1.0\n",
    "        control.steer = 0.0\n",
    "        return control, {}\n",
    "    \n",
    "    # Analyze projection\n",
    "    analysis = analyze_lidar_projection(front_projection)\n",
    "    \n",
    "    # Compute controls\n",
    "    steering = compute_steering(analysis)\n",
    "    throttle, brake = compute_throttle_brake(analysis)\n",
    "    \n",
    "    # Create control\n",
    "    control = carla.VehicleControl()\n",
    "    control.throttle = throttle\n",
    "    control.brake = brake\n",
    "    control.steer = steering\n",
    "    control.hand_brake = False\n",
    "    control.manual_gear_shift = False\n",
    "    \n",
    "    return control, analysis\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AI TRAJECTORY CONTROLLER READY\")\n",
    "print(\"=\"*60)\n",
    "print(\"Vehicle will be controlled by AI based on LiDAR projections\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b28904",
   "metadata": {},
   "source": [
    "## Step 7: Run Simulation with Continuous Spectator Updates (Synchronous)\n",
    "\n",
    "The vehicle will drive on autopilot while the spectator follows it in third-person view.\n",
    "In synchronous mode, the simulation advances only when `world.tick()` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01abc856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Autopilot DISABLED - AI controller taking over\n",
      "\n",
      "Running AI-controlled simulation for 30 seconds (600 ticks)...\n",
      "Watch the CARLA simulator window to see AI driving!\n",
      "\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "Frame 130: Saved 3 cameras + LiDAR (24024 points) + Projections (15052 projected)\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "\n",
      "Time: 5s | Speed: 18.4 km/h | Location: (-41.5, -10.5, 0.0)\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "Frame 140: Saved 3 cameras + LiDAR (23627 points) + Projections (14894 projected)\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "\n",
      "Time: 10s | Speed: 18.2 km/h | Location: (-29.4, -24.6, 0.2)\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "Frame 150: Saved 3 cameras + LiDAR (23816 points) + Projections (15031 projected)\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "\n",
      "Time: 15s | Speed: 18.2 km/h | Location: (-46.8, -18.3, 0.0)\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "Frame 160: Saved 3 cameras + LiDAR (23550 points) + Projections (14862 projected)\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "\n",
      "Time: 20s | Speed: 18.3 km/h | Location: (-28.5, -15.4, 0.2)\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "Frame 170: Saved 3 cameras + LiDAR (23928 points) + Projections (15021 projected)\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "\n",
      "Time: 25s | Speed: 18.3 km/h | Location: (-43.1, -26.9, 0.0)\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "  AI Control - Steer: -0.300, Throttle: 0.30, Brake: 0.00, Center density: 1.000\n",
      "\n",
      "Simulation interrupted by user\n",
      "\n",
      "AI-controlled simulation completed!\n",
      "Vehicle drove autonomously using LiDAR projection analysis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# DISABLE AUTOPILOT - AI takes control\n",
    "vehicle.set_autopilot(False)\n",
    "print(\"\\nAutopilot DISABLED - AI controller taking over\")\n",
    "\n",
    "simulation_duration = 30\n",
    "total_ticks = int(simulation_duration / fixed_delta)\n",
    "\n",
    "print(f\"\\nRunning AI-controlled simulation for {simulation_duration} seconds ({total_ticks} ticks)...\")\n",
    "print(\"Watch the CARLA simulator window to see AI driving!\\n\")\n",
    "\n",
    "tick_count = 0\n",
    "last_print_time = 0\n",
    "current_projection_images = {}\n",
    "\n",
    "try:\n",
    "    while tick_count < total_ticks:\n",
    "        # Advance simulation\n",
    "        world.tick()\n",
    "        tick_count += 1\n",
    "        \n",
    "        # Wait for sensor data\n",
    "        timeout = 2.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < timeout:\n",
    "            all_ready = True\n",
    "            for cam_name in cameras.keys():\n",
    "                if camera_queues[cam_name].qsize() == 0:\n",
    "                    all_ready = False\n",
    "                    break\n",
    "            if lidar_queue.qsize() == 0:\n",
    "                all_ready = False\n",
    "            \n",
    "            if all_ready:\n",
    "                break\n",
    "            time.sleep(0.001)\n",
    "        \n",
    "        # Process sensor data\n",
    "        should_save = (tick_count % capture_interval_ticks == 0)\n",
    "        sim_time = tick_count * fixed_delta\n",
    "        metadata, projection_images = process_sensor_queues_with_projections(should_save, sim_time)\n",
    "        \n",
    "        # Update projection images for AI\n",
    "        if projection_images:\n",
    "            current_projection_images = projection_images\n",
    "        \n",
    "        # AI CONTROL\n",
    "        if current_projection_images:\n",
    "            control, analysis = get_ai_control(current_projection_images)\n",
    "            vehicle.apply_control(control)\n",
    "            \n",
    "            # Print AI decisions\n",
    "            if tick_count % 20 == 0:\n",
    "                print(f\"  AI Control - Steer: {control.steer:+.3f}, \"\n",
    "                      f\"Throttle: {control.throttle:.2f}, \"\n",
    "                      f\"Brake: {control.brake:.2f}, \"\n",
    "                      f\"Center density: {analysis['close_center_density']:.3f}\")\n",
    "        \n",
    "        # Update spectator\n",
    "        update_spectator_view()\n",
    "        \n",
    "        # Get vehicle info\n",
    "        vehicle_location = vehicle.get_location()\n",
    "        vehicle_velocity = vehicle.get_velocity()\n",
    "        speed_kmh = 3.6 * np.sqrt(vehicle_velocity.x**2 + vehicle_velocity.y**2 + vehicle_velocity.z**2)\n",
    "        \n",
    "        # Print status\n",
    "        current_sim_time = tick_count * fixed_delta\n",
    "        if int(current_sim_time) % 5 == 0 and int(current_sim_time) != last_print_time:\n",
    "            last_print_time = int(current_sim_time)\n",
    "            print(f\"\\nTime: {int(current_sim_time)}s | \"\n",
    "                  f\"Speed: {speed_kmh:.1f} km/h | \"\n",
    "                  f\"Location: ({vehicle_location.x:.1f}, {vehicle_location.y:.1f}, {vehicle_location.z:.1f})\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nSimulation interrupted by user\")\n",
    "\n",
    "print(\"\\nAI-controlled simulation completed!\")\n",
    "print(f\"Vehicle drove autonomously using LiDAR projection analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc65af",
   "metadata": {},
   "source": [
    "## Step 8: Cleanup - Destroy All Spawned Actors\n",
    "\n",
    "This will remove the vehicle, cameras, and LiDAR we created, and restore asynchronous mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "010ab1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored original world settings (asynchronous mode)\n",
      "Stopped camera 'front' listener\n",
      "Stopped camera 'left' listener\n",
      "Stopped camera 'right' listener\n",
      "Stopped LiDAR listener\n",
      "Camera 'front' destroyed\n",
      "Camera 'left' destroyed\n",
      "Camera 'right' destroyed\n",
      "LiDAR destroyed\n",
      "Vehicle destroyed\n",
      "\n",
      "Cleanup completed!\n",
      "All spawned actors have been removed from the simulation\n",
      "\n",
      "Recorded data saved in: carla_output/\n",
      "  - Camera front images: carla_output/camera_front/\n",
      "  - Camera left images: carla_output/camera_left/\n",
      "  - Camera right images: carla_output/camera_right/\n",
      "  - LiDAR point clouds: carla_output/lidar/\n",
      "  - LiDAR projections: carla_output/lidar_projection/\n",
      "  - Fusion metadata: carla_output/fusion/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4084"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restore original settings (disable synchronous mode) before cleanup\n",
    "world.apply_settings(original_settings)\n",
    "traffic_manager.set_synchronous_mode(False)\n",
    "print(\"Restored original world settings (asynchronous mode)\")\n",
    "\n",
    "# Stop all sensor listeners first\n",
    "for cam_name, cam in cameras.items():\n",
    "    if cam is not None:\n",
    "        cam.stop()\n",
    "        print(f\"Stopped camera '{cam_name}' listener\")\n",
    "\n",
    "if lidar is not None:\n",
    "    lidar.stop()\n",
    "    print(\"Stopped LiDAR listener\")\n",
    "\n",
    "# Give a moment for listeners to finish\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Destroy all cameras\n",
    "for cam_name, cam in cameras.items():\n",
    "    if cam is not None:\n",
    "        cam.destroy()\n",
    "        print(f\"Camera '{cam_name}' destroyed\")\n",
    "\n",
    "# Destroy the LiDAR\n",
    "if lidar is not None:\n",
    "    lidar.destroy()\n",
    "    print(\"LiDAR destroyed\")\n",
    "\n",
    "# Destroy the vehicle\n",
    "if vehicle is not None:\n",
    "    vehicle.destroy()\n",
    "    print(\"Vehicle destroyed\")\n",
    "\n",
    "print(\"\\nCleanup completed!\")\n",
    "print(\"All spawned actors have been removed from the simulation\")\n",
    "print(f\"\\nRecorded data saved in: {output_base_dir}/\")\n",
    "print(f\"  - Camera front images: {camera_dirs['front']}/\")\n",
    "print(f\"  - Camera left images: {camera_dirs['left']}/\")\n",
    "print(f\"  - Camera right images: {camera_dirs['right']}/\")\n",
    "print(f\"  - LiDAR point clouds: {lidar_dir}/\")\n",
    "print(f\"  - LiDAR projections: {projection_dir}/\")\n",
    "print(f\"  - Fusion metadata: {fusion_dir}/\")\n",
    "world.tick()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd3184-1e74-47fa-8006-4c4025da8ef0",
   "metadata": {},
   "source": [
    "## Optional Tweaks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983083f-3cf5-4991-a80e-f859976164f8",
   "metadata": {},
   "source": [
    "Disable Autopilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae3148-8dc5-4f3f-ab0c-b5502eaf9311",
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle.set_autopilot(False)\n",
    "\n",
    "print(\"Autopilot disabled!\")\n",
    "world.tick()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
