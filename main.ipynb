{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a0f4574",
   "metadata": {},
   "source": [
    "# CARLA Autonomous Vehicle with LiDAR-Camera Fusion\n",
    "\n",
    "**AI-controlled vehicle using LiDAR-to-camera projection for obstacle detection and navigation**\n",
    "\n",
    "## Pipeline\n",
    "1. Connect to CARLA in synchronous mode\n",
    "2. Clear environment (keep only roads)\n",
    "3. Spawn vehicle with cameras + LiDAR\n",
    "4. Setup sensor fusion with LiDAR projection\n",
    "5. Run AI-controlled navigation\n",
    "6. Save synchronized sensor data\n",
    "7. Cleanup and restore\n",
    "\n",
    "See `README.md` for detailed documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fc85f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dependencies loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11056/1756308635.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  VIRIDIS = np.array(cm.get_cmap('viridis').colors)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import carla\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import queue\n",
    "from matplotlib import cm\n",
    "\n",
    "# Viridis colormap for LiDAR intensity visualization\n",
    "VIRIDIS = np.array(cm.get_cmap('viridis').colors)\n",
    "VID_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "\n",
    "print(\"‚úì Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0a9a82",
   "metadata": {},
   "source": [
    "## 1. CARLA Connection & Synchronous Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54b9efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Connected to CARLA\n"
     ]
    }
   ],
   "source": [
    "# Connect to CARLA server\n",
    "client = carla.Client('localhost', 2000)\n",
    "client.set_timeout(20.0)\n",
    "print(\"‚úì Connected to CARLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bb1fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Map: Carla/Maps/Town10HD_Opt\n",
      "‚úì Synchronous mode: 20 FPS\n"
     ]
    }
   ],
   "source": [
    "# Get world and store original settings\n",
    "world = client.get_world()\n",
    "carla_map = world.get_map()\n",
    "original_settings = world.get_settings()\n",
    "\n",
    "# Enable synchronous mode for deterministic sensor capture\n",
    "settings = world.get_settings()\n",
    "settings.synchronous_mode = True\n",
    "settings.fixed_delta_seconds = 0.05  # 20 FPS\n",
    "world.apply_settings(settings)\n",
    "\n",
    "# Synchronize traffic manager\n",
    "traffic_manager = client.get_trafficmanager(8000)\n",
    "traffic_manager.set_synchronous_mode(True)\n",
    "\n",
    "print(f\"‚úì Map: {carla_map.name}\")\n",
    "print(f\"‚úì Synchronous mode: {1/settings.fixed_delta_seconds:.0f} FPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e7080",
   "metadata": {},
   "source": [
    "## 2. Clear Environment (Keep Only Roads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5a725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all static map objects\n",
    "for layer in [carla.MapLayer.Buildings, carla.MapLayer.Decals, carla.MapLayer.Foliage,\n",
    "              carla.MapLayer.ParkedVehicles, carla.MapLayer.Particles, \n",
    "              carla.MapLayer.Props, carla.MapLayer.Walls]:\n",
    "    world.unload_map_layer(layer)\n",
    "world.tick()\n",
    "\n",
    "print(\"‚úì Environment cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ac8bb",
   "metadata": {},
   "source": [
    "## 3. Spawn Vehicle & Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49062329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Vehicle spawned at (-41.9, -30.4)\n"
     ]
    }
   ],
   "source": [
    "# Spawn vehicle at random location\n",
    "blueprint_library = world.get_blueprint_library()\n",
    "vehicle_bp = blueprint_library.filter('vehicle.tesla.model3')[0]\n",
    "spawn_point = random.choice(carla_map.get_spawn_points())\n",
    "vehicle = world.spawn_actor(vehicle_bp, spawn_point)\n",
    "world.tick()\n",
    "\n",
    "print(f\"‚úì Vehicle spawned at ({vehicle.get_location().x:.1f}, {vehicle.get_location().y:.1f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56c8c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì 3 cameras attached (1920x1080, 90¬∞ FOV)\n"
     ]
    }
   ],
   "source": [
    "# Configure RGB cameras (1920x1080, 90¬∞ FOV)\n",
    "camera_bp = blueprint_library.find('sensor.camera.rgb')\n",
    "camera_bp.set_attribute('image_size_x', '1920')\n",
    "camera_bp.set_attribute('image_size_y', '1080')\n",
    "camera_bp.set_attribute('fov', '90')\n",
    "\n",
    "# Camera positions: front, left, right\n",
    "camera_configs = {\n",
    "    'front': carla.Transform(carla.Location(x=2.5, z=1.0), carla.Rotation(pitch=0, yaw=0)),\n",
    "    'left': carla.Transform(carla.Location(x=0.0, y=-1.0, z=1.0), carla.Rotation(pitch=0, yaw=-90)),\n",
    "    'right': carla.Transform(carla.Location(x=0.0, y=1.0, z=1.0), carla.Rotation(pitch=0, yaw=90))\n",
    "}\n",
    "\n",
    "# Spawn and attach cameras\n",
    "cameras = {}\n",
    "for cam_name, transform in camera_configs.items():\n",
    "    cameras[cam_name] = world.spawn_actor(camera_bp, transform, attach_to=vehicle)\n",
    "world.tick()\n",
    "\n",
    "print(f\"‚úì {len(cameras)} cameras attached (1920x1080, 90¬∞ FOV)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f58bdcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Camera intrinsics computed (focal=960.0px)\n"
     ]
    }
   ],
   "source": [
    "# Compute camera intrinsic matrix K for LiDAR projection\n",
    "image_w = int(camera_bp.get_attribute(\"image_size_x\").as_int())\n",
    "image_h = int(camera_bp.get_attribute(\"image_size_y\").as_int())\n",
    "fov = float(camera_bp.get_attribute(\"fov\").as_float())\n",
    "focal = image_w / (2.0 * np.tan(fov * np.pi / 360.0))\n",
    "\n",
    "K = np.identity(3)\n",
    "K[0, 0] = K[1, 1] = focal\n",
    "K[0, 2] = image_w / 2.0\n",
    "K[1, 2] = image_h / 2.0\n",
    "\n",
    "print(f\"‚úì Camera intrinsics computed (focal={focal:.1f}px)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f466d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì LiDAR attached (64ch, 100m, 20Hz)\n"
     ]
    }
   ],
   "source": [
    "# Configure and attach LiDAR (64 channels, 100m range, 20Hz)\n",
    "lidar_bp = blueprint_library.find('sensor.lidar.ray_cast')\n",
    "lidar_bp.set_attribute('channels', '64')\n",
    "lidar_bp.set_attribute('points_per_second', '1200000')\n",
    "lidar_bp.set_attribute('rotation_frequency', '20')\n",
    "lidar_bp.set_attribute('range', '100')\n",
    "lidar_bp.set_attribute('upper_fov', '10')\n",
    "lidar_bp.set_attribute('lower_fov', '-30')\n",
    "\n",
    "lidar_transform = carla.Transform(carla.Location(x=0.0, z=2.5), carla.Rotation(pitch=0, yaw=0))\n",
    "lidar = world.spawn_actor(lidar_bp, lidar_transform, attach_to=vehicle)\n",
    "world.tick()\n",
    "\n",
    "print(\"‚úì LiDAR attached (64ch, 100m, 20Hz)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02832381",
   "metadata": {},
   "source": [
    "## 4. Spectator View Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e23e631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Spectator view configured\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14023"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup third-person spectator view\n",
    "spectator = world.get_spectator()\n",
    "\n",
    "def update_spectator_view():\n",
    "    \"\"\"Position spectator behind and above vehicle\"\"\"\n",
    "    transform = vehicle.get_transform()\n",
    "    spectator.set_transform(carla.Transform(\n",
    "        transform.location + carla.Location(x=0, z=4),\n",
    "        carla.Rotation(pitch=-15, yaw=transform.rotation.yaw)\n",
    "    ))\n",
    "\n",
    "update_spectator_view()\n",
    "print(\"‚úì Spectator view configured\")\n",
    "world.tick()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1767598",
   "metadata": {},
   "source": [
    "## 5. Sensor Fusion: LiDAR-to-Camera Projection\n",
    "\n",
    "Projects 3D LiDAR points onto 2D camera images using camera intrinsics. Synchronized queues ensure temporal alignment across all sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "545b87b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Output directories created: carla_output/\n",
      "‚úì Capture interval: 0.5s (10 ticks)\n",
      "‚úì All sensors streaming to queues\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SETUP OUTPUT DIRECTORIES & SENSOR QUEUES\n",
    "# ============================================================================\n",
    "output_base_dir = \"carla_output\"\n",
    "camera_dirs = {name: os.path.join(output_base_dir, f\"camera_{name}\") for name in cameras.keys()}\n",
    "lidar_dir = os.path.join(output_base_dir, \"lidar\")\n",
    "projection_dir = os.path.join(output_base_dir, \"lidar_projection\")\n",
    "fusion_dir = os.path.join(output_base_dir, \"fusion\")\n",
    "\n",
    "for directory in list(camera_dirs.values()) + [lidar_dir, projection_dir, fusion_dir]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Synchronous sensor queues\n",
    "camera_queues = {cam_name: queue.Queue() for cam_name in cameras.keys()}\n",
    "lidar_queue = queue.Queue()\n",
    "frame_counter = 0\n",
    "\n",
    "# Capture configuration\n",
    "fixed_delta = world.get_settings().fixed_delta_seconds\n",
    "capture_interval_seconds = 0.5\n",
    "capture_interval_ticks = int(capture_interval_seconds / fixed_delta)\n",
    "DOT_EXTENT = 2  # LiDAR projection dot size\n",
    "\n",
    "print(f\"‚úì Output directories created: {output_base_dir}/\")\n",
    "print(f\"‚úì Capture interval: {capture_interval_seconds}s ({capture_interval_ticks} ticks)\")\n",
    "\n",
    "# ============================================================================\n",
    "# SENSOR CALLBACKS\n",
    "# ============================================================================\n",
    "def create_image_callback(cam_name):\n",
    "    def process_image(image):\n",
    "        camera_queues[cam_name].put(image)\n",
    "    return process_image\n",
    "\n",
    "def lidar_callback(point_cloud):\n",
    "    lidar_queue.put(point_cloud)\n",
    "\n",
    "# ============================================================================\n",
    "# LIDAR-TO-CAMERA PROJECTION\n",
    "# ============================================================================\n",
    "def project_lidar_to_camera(lidar_data, camera, im_array):\n",
    "    \"\"\"Project 3D LiDAR points onto 2D camera image using intrinsics\"\"\"\n",
    "    # Parse point cloud (x, y, z, intensity)\n",
    "    p_cloud = np.frombuffer(lidar_data.raw_data, dtype=np.dtype('f4')).reshape((-1, 4))\n",
    "    intensity = p_cloud[:, 3]\n",
    "    \n",
    "    # Transform: LiDAR -> World -> Camera\n",
    "    local_points = np.r_[p_cloud[:, :3].T, [np.ones(len(p_cloud))]]\n",
    "    world_points = np.dot(lidar.get_transform().get_matrix(), local_points)\n",
    "    sensor_points = np.dot(np.array(camera.get_transform().get_inverse_matrix()), world_points)\n",
    "    \n",
    "    # Convert to camera coordinates\n",
    "    camera_coords = np.array([sensor_points[1], sensor_points[2] * -1, sensor_points[0]])\n",
    "    \n",
    "    # Project to 2D\n",
    "    points_2d = np.dot(K, camera_coords)\n",
    "    points_2d = np.array([points_2d[0] / points_2d[2], points_2d[1] / points_2d[2], points_2d[2]]).T\n",
    "    \n",
    "    # Filter points in image bounds\n",
    "    mask = (points_2d[:, 0] > 0) & (points_2d[:, 0] < image_w) & \\\n",
    "           (points_2d[:, 1] > 0) & (points_2d[:, 1] < image_h) & \\\n",
    "           (points_2d[:, 2] > 0)\n",
    "    points_2d, intensity = points_2d[mask], intensity[mask]\n",
    "    \n",
    "    # Colorize by intensity (Viridis colormap)\n",
    "    intensity = np.clip(1.0 - (4 * intensity - 3), 0.0, 1.0)\n",
    "    colors = np.array([\n",
    "        np.interp(intensity, VID_RANGE, VIRIDIS[:, 2]) * 255.0,\n",
    "        np.interp(intensity, VID_RANGE, VIRIDIS[:, 1]) * 255.0,\n",
    "        np.interp(intensity, VID_RANGE, VIRIDIS[:, 0]) * 255.0\n",
    "    ]).astype(np.uint8).T\n",
    "    \n",
    "    # Draw points on image\n",
    "    u, v = points_2d[:, 0].astype(int), points_2d[:, 1].astype(int)\n",
    "    for i in range(len(points_2d)):\n",
    "        v_min, v_max = max(0, v[i] - DOT_EXTENT), min(image_h, v[i] + DOT_EXTENT + 1)\n",
    "        u_min, u_max = max(0, u[i] - DOT_EXTENT), min(image_w, u[i] + DOT_EXTENT + 1)\n",
    "        im_array[v_min:v_max, u_min:u_max] = colors[i]\n",
    "    \n",
    "    return im_array, len(points_2d)\n",
    "\n",
    "# ============================================================================\n",
    "# DATA SAVING FUNCTIONS\n",
    "# ============================================================================\n",
    "def save_camera_image(cam_name, image, frame_num):\n",
    "    array = np.frombuffer(image.raw_data, dtype=np.uint8).reshape((image.height, image.width, 4))[:, :, :3]\n",
    "    filename = os.path.join(camera_dirs[cam_name], f\"frame_{frame_num:06d}.jpg\")\n",
    "    cv2.imwrite(filename, array)\n",
    "    return filename, array\n",
    "\n",
    "def save_projected_image(cam_name, im_array, frame_num):\n",
    "    proj_dir = os.path.join(projection_dir, f\"camera_{cam_name}\")\n",
    "    os.makedirs(proj_dir, exist_ok=True)\n",
    "    filename = os.path.join(proj_dir, f\"frame_{frame_num:06d}.jpg\")\n",
    "    cv2.imwrite(filename, im_array)\n",
    "    return filename\n",
    "\n",
    "def save_lidar_data(point_cloud, frame_num):\n",
    "    points = np.frombuffer(point_cloud.raw_data, dtype=np.float32).reshape((-1, 4))\n",
    "    filename = os.path.join(lidar_dir, f\"lidar_{frame_num:06d}.npy\")\n",
    "    np.save(filename, points)\n",
    "    return filename, len(points)\n",
    "\n",
    "def save_fusion_metadata(frame_num, camera_files, projection_files, lidar_file, \n",
    "                         lidar_points, projection_counts, timestamp):\n",
    "    import json\n",
    "    metadata = {\n",
    "        \"frame_id\": frame_num, \"timestamp\": timestamp,\n",
    "        \"cameras\": camera_files, \"lidar_projections\": projection_files,\n",
    "        \"projection_point_counts\": projection_counts,\n",
    "        \"lidar\": {\"file\": lidar_file, \"num_points\": lidar_points}\n",
    "    }\n",
    "    with open(os.path.join(fusion_dir, f\"frame_{frame_num:06d}.json\"), 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    return metadata\n",
    "\n",
    "# ============================================================================\n",
    "# SENSOR DATA PROCESSOR\n",
    "# ============================================================================\n",
    "def process_sensor_queues_with_projections(should_save=False, sim_time=0.0):\n",
    "    \"\"\"Process queues and return projection images for AI\"\"\"\n",
    "    global frame_counter\n",
    "    \n",
    "    # Clear queues if not saving\n",
    "    if not should_save:\n",
    "        for q in list(camera_queues.values()) + [lidar_queue]:\n",
    "            while not q.empty(): q.get()\n",
    "        return None, {}\n",
    "    \n",
    "    # Collect synchronized sensor data\n",
    "    camera_images = {}\n",
    "    for cam_name in cameras.keys():\n",
    "        if not camera_queues[cam_name].empty():\n",
    "            camera_images[cam_name] = camera_queues[cam_name].get()\n",
    "            while not camera_queues[cam_name].empty():\n",
    "                camera_queues[cam_name].get()\n",
    "    \n",
    "    lidar_data = lidar_queue.get() if not lidar_queue.empty() else None\n",
    "    while not lidar_queue.empty(): lidar_queue.get()\n",
    "    \n",
    "    # Process and save if all sensors ready\n",
    "    if len(camera_images) == len(cameras) and lidar_data is not None:\n",
    "        current_frame = frame_counter\n",
    "        frame_counter += 1\n",
    "        \n",
    "        # Save camera images\n",
    "        camera_files, camera_arrays = {}, {}\n",
    "        for cam_name, image in camera_images.items():\n",
    "            camera_files[cam_name], camera_arrays[cam_name] = save_camera_image(cam_name, image, current_frame)\n",
    "        \n",
    "        # Save LiDAR\n",
    "        lidar_file, num_points = save_lidar_data(lidar_data, current_frame)\n",
    "        \n",
    "        # Project and save\n",
    "        projection_files, projection_counts, projection_images = {}, {}, {}\n",
    "        for cam_name in cameras.keys():\n",
    "            proj_array = camera_arrays[cam_name].copy()\n",
    "            proj_array, num_proj = project_lidar_to_camera(lidar_data, cameras[cam_name], proj_array)\n",
    "            projection_files[cam_name] = save_projected_image(cam_name, proj_array, current_frame)\n",
    "            projection_counts[cam_name] = num_proj\n",
    "            projection_images[cam_name] = proj_array\n",
    "        \n",
    "        # Save metadata\n",
    "        save_fusion_metadata(current_frame, camera_files, projection_files, \n",
    "                           lidar_file, num_points, projection_counts, sim_time)\n",
    "        \n",
    "        if current_frame % 10 == 0:\n",
    "            print(f\"Frame {current_frame}: {num_points} LiDAR pts, {sum(projection_counts.values())} projected\")\n",
    "        \n",
    "        return None, projection_images\n",
    "    \n",
    "    return None, {}\n",
    "\n",
    "# Attach sensor listeners\n",
    "for cam_name, cam in cameras.items():\n",
    "    cam.listen(create_image_callback(cam_name))\n",
    "lidar.listen(lidar_callback)\n",
    "\n",
    "print(f\"‚úì All sensors streaming to queues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dee6bc-c7cb-4bdf-a6e6-6b08079a06fb",
   "metadata": {},
   "source": [
    "## 6. Custom Trajectory Model (Placeholder)\n",
    "\n",
    "Placeholder for custom vehicle control model. Receives LiDAR projection images each tick and outputs control commands. Currently inactive - vehicle uses CARLA autopilot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "950f399d-af90-4e37-b50c-afc868efa54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Custom trajectory model placeholder ready (inactive - using autopilot)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CUSTOM TRAJECTORY MODEL PLACEHOLDER\n",
    "# ============================================================================\n",
    "# TODO: Replace this with your own model for vehicle trajectory control\n",
    "#\n",
    "# Input:  projection_images (dict) - LiDAR projections for current tick\n",
    "#         - projection_images['front']  : np.ndarray (H, W, 3) - Front camera with LiDAR overlay\n",
    "#         - projection_images['left']   : np.ndarray (H, W, 3) - Left camera with LiDAR overlay  \n",
    "#         - projection_images['right']  : np.ndarray (H, W, 3) - Right camera with LiDAR overlay\n",
    "#\n",
    "# Output: carla.VehicleControl object with:\n",
    "#         - throttle (float): 0.0 to 1.0\n",
    "#         - brake (float): 0.0 to 1.0\n",
    "#         - steer (float): -1.0 (left) to 1.0 (right)\n",
    "#\n",
    "# This function is called every tick during simulation.\n",
    "# Return None to use CARLA autopilot (current behavior).\n",
    "# ============================================================================\n",
    "\n",
    "def custom_trajectory_model(projection_images):\n",
    "    \"\"\"\n",
    "    Custom trajectory model placeholder.\n",
    "    \n",
    "    Args:\n",
    "        projection_images: dict with 'front', 'left', 'right' LiDAR projection images\n",
    "        \n",
    "    Returns:\n",
    "        carla.VehicleControl: Control commands for the vehicle\n",
    "        None: Use CARLA autopilot instead (current default)\n",
    "    \"\"\"\n",
    "    # Extract projection images (available for your model)\n",
    "    front_proj = projection_images.get('front')  # Shape: (1080, 1920, 3)\n",
    "    left_proj = projection_images.get('left')    # Shape: (1080, 1920, 3)\n",
    "    right_proj = projection_images.get('right')  # Shape: (1080, 1920, 3)\n",
    "    \n",
    "    # -----------------------------------------------------------------\n",
    "    # TODO: Add your model inference here\n",
    "    # Example:\n",
    "    #   prediction = your_model.predict(front_proj, left_proj, right_proj)\n",
    "    #   control = carla.VehicleControl()\n",
    "    #   control.throttle = prediction['throttle']\n",
    "    #   control.steer = prediction['steer']\n",
    "    #   control.brake = prediction['brake']\n",
    "    #   return control\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    # Return None to use CARLA autopilot\n",
    "    return None\n",
    "\n",
    "print(\"‚úì Custom trajectory model placeholder ready (inactive - using autopilot)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b28904",
   "metadata": {},
   "source": [
    "## 7. Run Simulation (Autopilot)\n",
    "\n",
    "Vehicle drives using CARLA autopilot. Each tick, LiDAR projections are passed to the custom model placeholder for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01abc856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Autopilot enabled for 30s (600 ticks)\n",
      "Watch CARLA window for autonomous driving\n",
      "\n",
      "Frame 10: 31617 LiDAR pts, 20900 projected\n",
      "[5s] Speed: 29.0 km/h | Pos: (-35.2, -55.7)\n",
      "Frame 20: 31330 LiDAR pts, 20707 projected\n",
      "[10s] Speed: 29.0 km/h | Pos: (4.7, -57.4)\n",
      "Frame 30: 31325 LiDAR pts, 20569 projected\n",
      "[15s] Speed: 29.0 km/h | Pos: (45.0, -57.4)\n",
      "Frame 40: 31527 LiDAR pts, 20917 projected\n",
      "[20s] Speed: 29.0 km/h | Pos: (84.2, -51.5)\n",
      "Frame 50: 31577 LiDAR pts, 20413 projected\n",
      "[25s] Speed: 21.9 km/h | Pos: (99.4, -18.4)\n",
      "Frame 60: 31666 LiDAR pts, 20468 projected\n",
      "[30s] Speed: 0.0 km/h | Pos: (99.4, -8.4)\n",
      "\n",
      "‚úì Simulation completed!\n"
     ]
    }
   ],
   "source": [
    "# Enable CARLA autopilot\n",
    "vehicle.set_autopilot(True)\n",
    "simulation_duration = 30\n",
    "total_ticks = int(simulation_duration / fixed_delta)\n",
    "\n",
    "print(f\"‚úì Autopilot enabled for {simulation_duration}s ({total_ticks} ticks)\")\n",
    "print(\"Watch CARLA window for autonomous driving\\n\")\n",
    "\n",
    "tick_count = 0\n",
    "last_print_time = 0\n",
    "current_projection_images = {}\n",
    "\n",
    "try:\n",
    "    while tick_count < total_ticks:\n",
    "        world.tick()\n",
    "        tick_count += 1\n",
    "        \n",
    "        # Wait for synchronized sensor data (timeout 2s)\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < 2.0:\n",
    "            if all(camera_queues[c].qsize() > 0 for c in cameras.keys()) and lidar_queue.qsize() > 0:\n",
    "                break\n",
    "            time.sleep(0.001)\n",
    "        \n",
    "        # Process sensor data\n",
    "        should_save = (tick_count % capture_interval_ticks == 0)\n",
    "        _, projection_images = process_sensor_queues_with_projections(should_save, tick_count * fixed_delta)\n",
    "        \n",
    "        if projection_images:\n",
    "            current_projection_images = projection_images\n",
    "        \n",
    "        # Pass projection images to custom model (placeholder)\n",
    "        # When model returns control commands, they will override autopilot\n",
    "        if current_projection_images:\n",
    "            custom_control = custom_trajectory_model(current_projection_images)\n",
    "            \n",
    "            if custom_control is not None:\n",
    "                # Custom model returned control - disable autopilot and apply\n",
    "                vehicle.set_autopilot(False)\n",
    "                vehicle.apply_control(custom_control)\n",
    "        \n",
    "        # Update spectator view\n",
    "        update_spectator_view()\n",
    "        \n",
    "        # Print status every 5s\n",
    "        current_sim_time = tick_count * fixed_delta\n",
    "        if int(current_sim_time) % 5 == 0 and int(current_sim_time) != last_print_time:\n",
    "            last_print_time = int(current_sim_time)\n",
    "            loc = vehicle.get_location()\n",
    "            vel = vehicle.get_velocity()\n",
    "            speed = 3.6 * np.sqrt(vel.x**2 + vel.y**2 + vel.z**2)\n",
    "            print(f\"[{int(current_sim_time)}s] Speed: {speed:.1f} km/h | Pos: ({loc.x:.1f}, {loc.y:.1f})\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚úó Simulation interrupted by user\")\n",
    "\n",
    "print(\"\\n‚úì Simulation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dc65af",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "Stop sensors, destroy actors, and restore asynchronous mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010ab1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Cleanup completed\n",
      "\n",
      "üìÅ Data saved in: carla_output/\n",
      "   ‚îú‚îÄ‚îÄ camera_front/left/right/  (raw images)\n",
      "   ‚îú‚îÄ‚îÄ lidar/                     (point clouds)\n",
      "   ‚îú‚îÄ‚îÄ lidar_projection/          (projected images)\n",
      "   ‚îî‚îÄ‚îÄ fusion/                    (JSON metadata)\n"
     ]
    }
   ],
   "source": [
    "# Restore async mode\n",
    "world.apply_settings(original_settings)\n",
    "traffic_manager.set_synchronous_mode(False)\n",
    "\n",
    "# Stop sensor listeners\n",
    "for cam in cameras.values():\n",
    "    cam.stop()\n",
    "lidar.stop()\n",
    "time.sleep(0.5)\n",
    "\n",
    "# Destroy actors\n",
    "for cam in cameras.values():\n",
    "    cam.destroy()\n",
    "lidar.destroy()\n",
    "vehicle.destroy()\n",
    "world.tick()\n",
    "\n",
    "print(\"‚úì Cleanup completed\")\n",
    "print(f\"\\nüìÅ Data saved in: {output_base_dir}/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ camera_front/left/right/  (raw images)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ lidar/                     (point clouds)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ lidar_projection/          (projected images)\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ fusion/                    (JSON metadata)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
